{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'spectral'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-bfa64bec4ae4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspectral\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named 'spectral'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import random\n",
    "from random import shuffle\n",
    "from skimage.transform import rotate\n",
    "import scipy.ndimage\n",
    "import scipy as sp\n",
    "from scipy import fftpack\n",
    "from keras.models import load_model\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "import spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadIndianPinesData():\n",
    "    data_path = os.path.join(os.getcwd(),'data')\n",
    "    data = sio.loadmat(\"../../../Datasets/Indian pines/Indian_pines_corrected.mat\")['indian_pines_corrected']\n",
    "    labels = sio.loadmat(\"../../../Datasets/Indian pines/Indian_pines_gt.mat\")['indian_pines_gt']\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "def splitTrainTestSet(X, y, testRatio=0.10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=345,\n",
    "                                                        stratify=y)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def dost_bw(l):\n",
    "    out = np.zeros(int(2*np.log2(l)))\n",
    "    l1 = np.arange(np.log2(l)-2,-1,-1)\n",
    "    l2 = np.arange(0,np.log2(l)-1)\n",
    "    out[1:int(1+np.log2(l)-1)]=l1\n",
    "    out[-int(np.log2(l)-1):]=l2\n",
    "    \n",
    "    out = np.exp2(out).astype(np.int16)\n",
    "    return out\n",
    "\n",
    "def dost(inp):\n",
    "    l = inp.shape[0]\n",
    "    fft_inp = fftpack.fftshift(fftpack.fft(fftpack.ifftshift(inp,axes=0),axis=0),axes=0)\n",
    "    #plt.figure(figsize = (30,5))\n",
    "    #ax = np.linspace(-512,511,2**10)\n",
    "    #plt.plot(ax,fft_inp[0,:])\n",
    "    bw_inp = dost_bw(l)\n",
    "#     print(bw_inp)\n",
    "    k = 0\n",
    "    dost_inp = np.zeros_like(fft_inp)\n",
    "\n",
    "    for r in bw_inp:\n",
    "        if(r==1):\n",
    "            dost_inp[k,:] = fft_inp[k,:]\n",
    "            k = k+r\n",
    "        else:\n",
    "            dost_inp[k:r+k,:] = fftpack.fftshift(fftpack.ifft(fftpack.ifftshift(fft_inp[k:r+k,:],axes=0),axis=0),axes=0)\n",
    "            k = k+r\n",
    "\n",
    "#     plt.plot(fft_inp)\n",
    "    #plt.figure(figsize = (20,5))\n",
    "    #plt.plot(np.abs(dost_inp[0,:]))\n",
    "    \n",
    "    \n",
    "    return dost_inp\n",
    "    \n",
    "    \n",
    "def oversampleWeakClasses(X, y):\n",
    "    uniqueLabels, labelCounts = np.unique(y, return_counts=True)\n",
    "    maxCount = np.max(labelCounts)\n",
    "    labelInverseRatios = maxCount / labelCounts  \n",
    "    # repeat for every label and concat\n",
    "    newX = X[y == uniqueLabels[0], :, :, :].repeat(round(labelInverseRatios[0]), axis=0)\n",
    "    newY = y[y == uniqueLabels[0]].repeat(round(labelInverseRatios[0]), axis=0)\n",
    "    for label, labelInverseRatio in zip(uniqueLabels[1:], labelInverseRatios[1:]):\n",
    "        cX = X[y== label,:,:,:].repeat(round(labelInverseRatio), axis=0)\n",
    "        cY = y[y == label].repeat(round(labelInverseRatio), axis=0)\n",
    "        newX = np.concatenate((newX, cX))\n",
    "        newY = np.concatenate((newY, cY))\n",
    "    np.random.seed(seed=42)\n",
    "    rand_perm = np.random.permutation(newY.shape[0])\n",
    "    newX = newX[rand_perm, :, :, :]\n",
    "    newY = newY[rand_perm]\n",
    "    return newX, newY\n",
    "\n",
    "\n",
    "def standartizeData(X):\n",
    "    newX = np.reshape(X, (-1, X.shape[2]))\n",
    "    scaler = preprocessing.StandardScaler().fit(newX)  \n",
    "    newX = scaler.transform(newX)\n",
    "    newX = np.reshape(newX, (X.shape[0],X.shape[1],X.shape[2]))\n",
    "    return newX, scaler\n",
    "\n",
    "def applyPCA(X, numComponents=75):\n",
    "    newX = np.reshape(X, (-1, X.shape[2]))\n",
    "    pca = PCA(n_components=numComponents, whiten=True)\n",
    "    newX = pca.fit_transform(newX)\n",
    "    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n",
    "    return newX, pca\n",
    "\n",
    "def padWithZeros(X, margin=2):\n",
    "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
    "    x_offset = margin\n",
    "    y_offset = margin\n",
    "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
    "    return newX\n",
    "\n",
    "def createPatches(X, y, windowSize=5, removeZeroLabels = True):\n",
    "    margin = int((windowSize - 1) / 2)\n",
    "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
    "    # split patches\n",
    "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n",
    "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
    "    patchIndex = 0\n",
    "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
    "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
    "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n",
    "            patchesData[patchIndex, :, :, :] = patch\n",
    "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
    "            patchIndex = patchIndex + 1\n",
    "    if removeZeroLabels:\n",
    "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
    "        patchesLabels = patchesLabels[patchesLabels>0]\n",
    "        patchesLabels -= 1\n",
    "    return patchesData, patchesLabels\n",
    "\n",
    "\n",
    "def AugmentData(X_train):\n",
    "    for i in range(int(X_train.shape[0]/2)):\n",
    "        patch = X_train[i,:,:,:]\n",
    "        num = random.randint(0,2)\n",
    "        if (num == 0):\n",
    "            \n",
    "            flipped_patch = np.flipud(patch)\n",
    "        if (num == 1):\n",
    "            \n",
    "            flipped_patch = np.fliplr(patch)\n",
    "        if (num == 2):\n",
    "            \n",
    "            no = random.randrange(-180,180,30)\n",
    "            flipped_patch = scipy.ndimage.interpolation.rotate(patch, no,axes=(1, 0),\n",
    "                                                               reshape=False, output=None, order=3, mode='constant', cval=0.0, prefilter=False)\n",
    "    \n",
    "    \n",
    "    patch2 = flipped_patch\n",
    "    X_train[i,:,:,:] = patch2\n",
    "    \n",
    "    return X_train\n",
    "\n",
    "\n",
    "def savePreprocessedData(X_trainPatches, X_testPatches, y_trainPatches, y_testPatches, windowSize,dost_applied=False, wasPCAapplied = False, numPCAComponents = 0, testRatio = 0.25):\n",
    "    if dost_applied:\n",
    "        if wasPCAapplied:\n",
    "            with open(\"../../data/XtrainWindowSize\" + str(windowSize) + \"PCA\" + str(numPCAComponents) + \"testRatio\" + str(testRatio) + \"_dost.npy\", 'wb+') as outfile:\n",
    "                np.save(outfile, X_trainPatches)\n",
    "            with open(\"../../data/XtestWindowSize\" + str(windowSize) + \"PCA\" + str(numPCAComponents) + \"testRatio\" + str(testRatio) + \"_dost.npy\", 'wb+') as outfile:\n",
    "                np.save(outfile, X_testPatches)\n",
    "            with open(\"../../data/ytrainWindowSize\" + str(windowSize) + \"PCA\" + str(numPCAComponents) + \"testRatio\" + str(testRatio) + \"_dost.npy\", 'wb+') as outfile:\n",
    "                np.save(outfile, y_trainPatches)\n",
    "            with open(\"../../data/ytestWindowSize\" + str(windowSize) + \"PCA\" + str(numPCAComponents) + \"testRatio\" + str(testRatio) + \"_dost.npy\", 'wb+') as outfile:\n",
    "                np.save(outfile, y_testPatches)\n",
    "        else:\n",
    "            with open(\"../../data/XtrainWindowSize\" + str(windowSize) + \"_dost.npy\", 'bw+') as outfile:\n",
    "                np.save(outfile, X_trainPatches)\n",
    "            with open(\"../../data/XtestWindowSize\" + str(windowSize) + \"_dost.npy\", 'bw+') as outfile:\n",
    "                np.save(outfile, X_testPatches)\n",
    "            with open(\"../../data/ytrainWindowSize\" + str(windowSize) + \"_dost.npy\", 'bw+') as outfile:\n",
    "                np.save(outfile, y_trainPatches)\n",
    "            with open(\"../../data/ytestWindowSize\" + str(windowSize) + \"_dost.npy\", 'bw+') as outfile:\n",
    "                np.save(outfile, y_testPatches)\n",
    "    else:\n",
    "        if wasPCAapplied:\n",
    "            with open(\"../../data/XtrainWindowSize\" + str(windowSize) + \"PCA\" + str(numPCAComponents) + \"testRatio\" + str(testRatio) + \".npy\", 'wb+') as outfile:\n",
    "                np.save(outfile, X_trainPatches)\n",
    "            with open(\"../../data/XtestWindowSize\" + str(windowSize) + \"PCA\" + str(numPCAComponents) + \"testRatio\" + str(testRatio) + \".npy\", 'wb+') as outfile:\n",
    "                np.save(outfile, X_testPatches)\n",
    "            with open(\"../../data/ytrainWindowSize\" + str(windowSize) + \"PCA\" + str(numPCAComponents) + \"testRatio\" + str(testRatio) + \".npy\", 'wb+') as outfile:\n",
    "                np.save(outfile, y_trainPatches)\n",
    "            with open(\"../../data/ytestWindowSize\" + str(windowSize) + \"PCA\" + str(numPCAComponents) + \"testRatio\" + str(testRatio) + \".npy\", 'wb+') as outfile:\n",
    "                np.save(outfile, y_testPatches)\n",
    "        else:\n",
    "            with open(\"../../data/XtrainWindowSize\" + str(windowSize) + \".npy\", 'bw+') as outfile:\n",
    "                np.save(outfile, X_trainPatches)\n",
    "            with open(\"../../data/XtestWindowSize\" + str(windowSize) + \".npy\", 'bw+') as outfile:\n",
    "                np.save(outfile, X_testPatches)\n",
    "            with open(\"../../data/ytrainWindowSize\" + str(windowSize) + \".npy\", 'bw+') as outfile:\n",
    "                np.save(outfile, y_trainPatches)\n",
    "            with open(\"../../data/ytestWindowSize\" + str(windowSize) + \".npy\", 'bw+') as outfile:\n",
    "                np.save(outfile, y_testPatches)\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "numComponents = 64\n",
    "windowSize = 7\n",
    "testRatio = 0.95\n",
    "dost_ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = loadIndianPinesData()\n",
    "print(\"Shape of X : \",end='')\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dost_:\n",
    "    X_dost = np.zeros([X.shape[0],X.shape[1],256])\n",
    "    X_dost[:,:,:200] = X\n",
    "\n",
    "    X_ = np.abs(dost(X_dost.reshape([-1,X_dost.shape[2]],order='F').T).T.reshape(X_dost.shape,order='F'))\n",
    "else:\n",
    "    X_ = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca,pca = applyPCA(X_,numComponents=numComponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XPatches, yPatches = createPatches(X_pca, y, windowSize=windowSize)\n",
    "X_train, X_test, y_train, y_test = splitTrainTestSet(XPatches, yPatches, testRatio)\n",
    "X_train, y_train = oversampleWeakClasses(X_train, y_train)\n",
    "X_train = AugmentData(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train : {}\".format(X_train.shape))\n",
    "print(\"y_train : {}\".format(y_train.shape))\n",
    "print(\"X_test : {}\".format(X_test.shape))\n",
    "print(\"y_test : {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,Conv3D,MaxPooling3D\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "#from sklearn.cross_validation import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(windowSize,testRatio,PCA_applied,DOST_applied,numPCAcomponents):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input Shape : {}\".format(X_train[...][0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(2*C1, (3, 3),data_format=\"channels_last\", activation='relu', input_shape=input_shape))\n",
    "model.add(Dropout(0.4))\n",
    "# model.add(MaxPooling3D(pool_size=()))\n",
    "model.add(Conv2D(C1, (3, 3), data_format=\"channels_last\",activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "# model.add(MaxPooling3D((2,2,2)))\n",
    "# model.add(Conv3D(C1, (2,3, 3), activation='relu'))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../models/conv_v1.h5\"\n",
    "history = model.fit(X_train,\n",
    "    y_train,\n",
    "    batch_size=16,\n",
    "    epochs=50,\n",
    "    #show_accuracy=False,\n",
    "#     verbose=2,\n",
    "#     validation_split = 0.1,\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='auto'),\n",
    "#         keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=0, mode='auto'),\n",
    "#         keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n",
    "    ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reports (X_test,y_test,model):\n",
    "    Y_pred = model.predict(X_test,batch_size=8)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    target_names = ['Alfalfa', 'Corn-notill', 'Corn-mintill', 'Corn'\n",
    "               ,'Grass-pasture', 'Grass-trees', 'Grass-pasture-mowed', \n",
    "                'Hay-windrowed', 'Oats', 'Soybean-notill', 'Soybean-mintill',\n",
    "               'Soybean-clean', 'Wheat', 'Woods', 'Buildings-Grass-Trees-Drives',\n",
    "               'Stone-Steel-Towers']\n",
    "\n",
    "    \n",
    "    classification = classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names)\n",
    "    confusion = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
    "    score = model.evaluate(X_test, y_test, batch_size=8)\n",
    "    Test_Loss =  score[0]*100\n",
    "    Test_accuracy = score[1]*100\n",
    "    \n",
    "    return classification, confusion, Test_Loss, Test_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_ = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification, confusion, Test_loss, Test_accuracy = reports(X_test,y_test_,model)\n",
    "classification = str(classification)\n",
    "confusion = str(confusion)\n",
    "numComponents = 64\n",
    "file_name = 'report' + \"WindowSize\" + str(windowSize) + \"PCA\" + str(numComponents) + \"testRatio\" + str(testRatio) +\"_dost.txt\"\n",
    "with open(file_name, 'w+') as x_file:\n",
    "    x_file.write('{} Test loss (%)'.format(Test_loss))\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('{} Test accuracy (%)'.format(Test_accuracy))\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('{}'.format(classification))\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('{}'.format(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
