{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# %load \"DOST based CNN for HSI classification.py\"\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import random\n",
    "from random import shuffle\n",
    "from skimage.transform import rotate\n",
    "import scipy.ndimage\n",
    "import scipy as sp\n",
    "from scipy import fftpack\n",
    "from keras.models import load_model\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "import spectral\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,Conv3D,MaxPooling3D\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "    \n",
    "    # In[7]:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadIndianPinesData():\n",
    "    data_path = os.path.join(os.getcwd(),'data')\n",
    "    data = sio.loadmat(\"../../Datasets/Indian pines/Indian_pines_corrected.mat\")['indian_pines_corrected']\n",
    "    labels = sio.loadmat(\"../../Datasets/Indian pines/Indian_pines_gt.mat\")['indian_pines_gt']\n",
    "    \n",
    "    return data, labels\n",
    "    \n",
    "\n",
    "def classifier(X,y,numComponents = 64, windowSize =7,testRatio = 0.95,dost_ = True,wls_alpha=1.0,wls_lambda=1.0,report=None,index=1):\n",
    "    \n",
    "#     reportpath = \"../reports/conv_{}_{}_{}_{}_{}_{}.txt\".format(numComponents,windowSize,testRatio,dost_,wls_alpha,wls_lambda)\n",
    "    \n",
    "#     with open(reportpath,'w+') as report:\n",
    "    \n",
    "#         print(\"\"\"         ------------------Iteration Info : ----------------------- \\n  \n",
    "#                                 Shape of X :    {}\\n\n",
    "#                                 Shape of y :    {}\\n\n",
    "#                                 numComponents : {}\\n\n",
    "#                                 windowSize :    {}\\n\n",
    "#                                 testRatio :     {}\\n\n",
    "#                                 use dost :      {}\\n\n",
    "#                                 wls_alpha :     {}\\n\n",
    "#                                 wls_lambda :    {}\\n\n",
    "#         \"\"\".format(X.shape,y.shape,numComponents,windowSize,testRatio,dost_,wls_alpha,wls_lambda))\n",
    "#         print(\"\"\"         ------------------Iteration Info : ----------------------- \\n  \n",
    "#                                 Shape of X :    {}\\n\n",
    "#                                 Shape of y :    {}\\n\n",
    "#                                 numComponents : {}\\n\n",
    "#                                 windowSize :    {}\\n\n",
    "#                                 testRatio :     {}\\n\n",
    "#                                 use dost :      {}\\n\n",
    "#                                 wls_alpha :     {}\\n\n",
    "#                                 wls_lambda :    {}\\n\n",
    "#         \"\"\".format(X.shape,y.shape,numComponents,windowSize,testRatio,dost_,wls_alpha,wls_lambda),file = report)\n",
    "        \n",
    "        \n",
    "    def splitTrainTestSet(X, y, testRatio):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=345,\n",
    "                                                            stratify=y)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "    def dost_bw(l):\n",
    "        out = np.zeros(int(2*np.log2(l)))\n",
    "        l1 = np.arange(np.log2(l)-2,-1,-1)\n",
    "        l2 = np.arange(0,np.log2(l)-1)\n",
    "        out[1:int(1+np.log2(l)-1)]=l1\n",
    "        out[-int(np.log2(l)-1):]=l2\n",
    "        out = np.exp2(out).astype(np.int16)\n",
    "        return out\n",
    "\n",
    "    def dost(inp):\n",
    "        l = inp.shape[0]\n",
    "        fft_inp = fftpack.fftshift(fftpack.fft(fftpack.ifftshift(inp,axes=0),axis=0),axes=0)\n",
    "        bw_inp = dost_bw(l)\n",
    "        k = 0\n",
    "        dost_inp = np.zeros_like(fft_inp)\n",
    "        for r in bw_inp:\n",
    "            if(r==1):\n",
    "                dost_inp[k,:] = fft_inp[k,:]\n",
    "                k = k+r\n",
    "            else:\n",
    "                dost_inp[k:r+k,:] = fftpack.fftshift(fftpack.ifft(fftpack.ifftshift(fft_inp[k:r+k,:],axes=0),axis=0),axes=0)\n",
    "                k = k+r\n",
    "        return dost_inp\n",
    "\n",
    "\n",
    "    def oversampleWeakClasses(X, y):\n",
    "        uniqueLabels, labelCounts = np.unique(y, return_counts=True)\n",
    "        maxCount = np.max(labelCounts)\n",
    "        labelInverseRatios = maxCount / labelCounts  \n",
    "        newX = X[y == uniqueLabels[0], :, :, :].repeat(round(labelInverseRatios[0]), axis=0)\n",
    "        newY = y[y == uniqueLabels[0]].repeat(round(labelInverseRatios[0]), axis=0)\n",
    "        for label, labelInverseRatio in zip(uniqueLabels[1:], labelInverseRatios[1:]):\n",
    "            cX = X[y== label,:,:,:].repeat(round(labelInverseRatio), axis=0)\n",
    "            cY = y[y == label].repeat(round(labelInverseRatio), axis=0)\n",
    "            newX = np.concatenate((newX, cX))\n",
    "            newY = np.concatenate((newY, cY))\n",
    "        np.random.seed(seed=42)\n",
    "        rand_perm = np.random.permutation(newY.shape[0])\n",
    "        newX = newX[rand_perm, :, :, :]\n",
    "        newY = newY[rand_perm]\n",
    "        return newX, newY\n",
    "\n",
    "\n",
    "    def standartizeData(X):\n",
    "        newX = np.reshape(X, (-1, X.shape[2]))\n",
    "        scaler = preprocessing.StandardScaler().fit(newX)  \n",
    "        newX = scaler.transform(newX)\n",
    "        newX = np.reshape(newX, (X.shape[0],X.shape[1],X.shape[2]))\n",
    "        return newX, scaler\n",
    "\n",
    "    def applyPCA(X, numComponents=75):\n",
    "        newX = np.reshape(X, (-1, X.shape[2]))\n",
    "        pca = PCA(n_components=numComponents, whiten=True)\n",
    "        newX = pca.fit_transform(newX)\n",
    "        newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n",
    "        return newX, pca\n",
    "\n",
    "    def padWithZeros(X, margin=2):\n",
    "        newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
    "        x_offset = margin\n",
    "        y_offset = margin\n",
    "        newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
    "        return newX\n",
    "\n",
    "    def createPatches(X, y, windowSize=5, removeZeroLabels = True):\n",
    "        margin = int((windowSize - 1) / 2)\n",
    "        zeroPaddedX = padWithZeros(X, margin=margin)\n",
    "        patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n",
    "        patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
    "        patchIndex = 0\n",
    "        for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
    "            for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
    "                patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n",
    "                patchesData[patchIndex, :, :, :] = patch\n",
    "                patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
    "                patchIndex = patchIndex + 1\n",
    "        if removeZeroLabels:\n",
    "            patchesData = patchesData[patchesLabels>0,:,:,:]\n",
    "            patchesLabels = patchesLabels[patchesLabels>0]\n",
    "            patchesLabels -= 1\n",
    "        return patchesData, patchesLabels\n",
    "\n",
    "\n",
    "    def AugmentData(X_train):\n",
    "        for i in range(int(X_train.shape[0]/2)):\n",
    "            patch = X_train[i,:,:,:]\n",
    "            num = random.randint(0,2)\n",
    "            if (num == 0):\n",
    "\n",
    "                flipped_patch = np.flipud(patch)\n",
    "            if (num == 1):\n",
    "\n",
    "                flipped_patch = np.fliplr(patch)\n",
    "            if (num == 2):\n",
    "\n",
    "                no = random.randrange(-180,180,30)\n",
    "                flipped_patch = scipy.ndimage.interpolation.rotate(patch, no,axes=(1, 0),\n",
    "                                                                   reshape=False, output=None, order=3, mode='constant', cval=0.0, prefilter=False)\n",
    "\n",
    "\n",
    "        patch2 = flipped_patch\n",
    "        X_train[i,:,:,:] = patch2\n",
    "\n",
    "        return X_train\n",
    "\n",
    "#    print(\"Shape of X : \",end='')\n",
    "#    print(X.shape)\n",
    "\n",
    "    if dost_:\n",
    "        X_dost = np.zeros([X.shape[0],X.shape[1],256])\n",
    "        X_dost[:,:,:200] = X\n",
    "\n",
    "        X_ = np.abs(dost(X_dost.reshape([-1,X_dost.shape[2]],order='F').T).T.reshape(X_dost.shape,order='F'))\n",
    "    else:\n",
    "        X_ = X\n",
    "\n",
    "\n",
    "    X_pca,pca = applyPCA(X_,numComponents=numComponents) \n",
    "    XPatches, yPatches = createPatches(X_pca, y, windowSize=windowSize)\n",
    "    X_train, X_test, y_train, y_test = splitTrainTestSet(XPatches, yPatches, testRatio)\n",
    "    X_test,X_val, y_test, y_val = splitTrainTestSet(X_test, y_test, 0.3)\n",
    "    if index ==1:\n",
    "        print(\"X_train before over sampling : {}\".format(X_train.shape),file = report)\n",
    "        print(\"y_train before over sampling : {}\".format(y_train.shape),file = report)\n",
    "    X_train, y_train = oversampleWeakClasses(X_train, y_train)\n",
    "    X_train = AugmentData(X_train)\n",
    "\n",
    "    if index==1:\n",
    "        print(\"X_train : {}\".format(X_train.shape),file = report)\n",
    "        print(\"y_train : {}\".format(y_train.shape),file = report)\n",
    "        print(\"X_test : {}\".format(X_test.shape),file = report)\n",
    "        print(\"y_test : {}\".format(X_test.shape),file = report)\n",
    "        print(\"X_val : {}\".format(X_val.shape),file = report)\n",
    "        print(\"y_val : {}\".format(y_val.shape),file = report)\n",
    "\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "    y_val = np_utils.to_categorical(y_val)\n",
    "    input_shape = X_train[0].shape\n",
    "#    print(\"Input Shape : {}\".format(X_train[...][0].shape))\n",
    "\n",
    "    C1 = 64\n",
    "\n",
    "    # Define the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(2*C1, (3, 3),data_format=\"channels_last\", activation='relu', input_shape=input_shape))\n",
    "    model.add(Dropout(0.4))\n",
    "    # model.add(MaxPooling3D(pool_size=()))\n",
    "    model.add(Conv2D(C1, (3, 3), data_format=\"channels_last\",activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    # model.add(MaxPooling3D((2,2,2)))\n",
    "    # model.add(Conv3D(C1, (2,3, 3), activation='relu'))\n",
    "    # model.add(Dropout(0.25))  \n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(16, activation='softmax'))\n",
    "\n",
    "#     print(model.summary())\n",
    "#        print(model.summary(),file = report)\n",
    "\n",
    "    sgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    filepath = \"../models/conv_{}_{}_{}_{}_{}_{}_{}.h5\".format(numComponents,windowSize,testRatio,dost_,wls_alpha,wls_lambda,index)\n",
    "#     print(filepath)\n",
    "    history = model.fit(X_train,\n",
    "        y_train,\n",
    "        batch_size=16,\n",
    "        epochs=100,\n",
    "        #show_accuracy=False,\n",
    "        verbose=None,\n",
    "        validation_data = (X_val,y_val),\n",
    "        callbacks = [\n",
    "            keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='auto'),\n",
    "             keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=0, mode='auto'),\n",
    "            keras.callbacks.RemoteMonitor(root='http://localhost:8888')\n",
    "#                 keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n",
    "        ]\n",
    "        )\n",
    "\n",
    "\n",
    "    model.save(filepath=filepath)\n",
    "\n",
    "    def reports (X_test,y_test,model):\n",
    "        Y_pred = model.predict(X_test,batch_size=8)\n",
    "        y_pred = np.argmax(Y_pred, axis=1)\n",
    "        target_names = ['Alfalfa', 'Corn-notill', 'Corn-mintill', 'Corn'\n",
    "                   ,'Grass-pasture', 'Grass-trees', 'Grass-pasture-mowed', \n",
    "                    'Hay-windrowed', 'Oats', 'Soybean-notill', 'Soybean-mintill',\n",
    "                   'Soybean-clean', 'Wheat', 'Woods', 'Buildings-Grass-Trees-Drives',\n",
    "                   'Stone-Steel-Towers']\n",
    "\n",
    "\n",
    "        classification = classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names)\n",
    "        confusion = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
    "        score = model.evaluate(X_test, y_test, batch_size=8)\n",
    "        Test_Loss =  score[0]*100\n",
    "        Test_accuracy = score[1]*100\n",
    "            \n",
    "        return classification, confusion, Test_Loss, Test_accuracy\n",
    "\n",
    "\n",
    "\n",
    "#    y_test_ = np_utils.to_categorical(y_test)\n",
    "#    \n",
    "#    classification, confusion, Test_loss, Test_accuracy = reports(X_test,y_test_,model)\n",
    "#    classification = str(classification)\n",
    "#    confusion = str(confusion)\n",
    "#    numComponents = 64\n",
    "#    file_name = 'report' + \"WindowSize\" + str(windowSize) + \"PCA\" + str(numComponents) + \"testRatio\" + str(testRatio) +\".txt\"\n",
    "#    with open(file_name, 'w+') as x_file:\n",
    "#        x_file.write('{} Test loss (%)'.format(Test_loss))\n",
    "#        x_file.write('\\n')\n",
    "#        x_file.write('{} Test accuracy (%)'.format(Test_accuracy))\n",
    "#        x_file.write('\\n')\n",
    "#        x_file.write('\\n')\n",
    "#        x_file.write('{}'.format(classification))\n",
    "#        x_file.write('\\n')\n",
    "#        x_file.write('{}'.format(confusion))\n",
    "\n",
    "    XPatches, yPatches = createPatches(X_pca, y, windowSize=windowSize,removeZeroLabels=False)\n",
    "    predictions = model.predict_classes(XPatches,batch_size=16)+1\n",
    "    pred_proba = model.predict_proba(XPatches,batch_size=16).reshape((145,145,16))   \n",
    "    predictions[yPatches==0] = 0\n",
    "    pred_map = predictions.reshape((145,145))\n",
    "    \n",
    "    \n",
    "\n",
    "    def wls_filter(img,lambda_,alpha,L):\n",
    "\n",
    "        epsilon = 1e-4\n",
    "        (r,c)=img.shape\n",
    "        k = r*c\n",
    "\n",
    "        dy = np.diff(L,1,0)\n",
    "        dy = -lambda_/(np.power(np.abs(dy),alpha)+epsilon)\n",
    "        dy = np.lib.pad(dy,[(0,1),(0,0)],'constant',constant_values=0)\n",
    "        dy = dy.flatten('F')\n",
    "\n",
    "        dx = np.diff(L,1,1)\n",
    "        dx = -lambda_/(np.power(np.abs(dx),alpha)+epsilon)\n",
    "        dx = np.lib.pad(dx,[(0,0),(0,1)],'constant',constant_values=0)\n",
    "        dx = dx.flatten('F')\n",
    "\n",
    "        B = np.array([dx,dy])\n",
    "        A = sp.sparse.diags(B,[-r,-1],(k,k))\n",
    "\n",
    "        e = dx\n",
    "        w = np.pad(dx,[(r,0)],'constant',constant_values=0)[:-r]\n",
    "        s = dy\n",
    "        n = np.pad(dy,[(1,0)],'constant',constant_values=0)[:-1]\n",
    "\n",
    "        D = 1-(e+w+s+n)\n",
    "\n",
    "        A = A + A.T + sp.sparse.diags(D.reshape(1,-1),[0],(k,k))\n",
    "\n",
    "        result = sp.sparse.linalg.spsolve(A,img.reshape((-1,1),order = 'F'))\n",
    "        result = result.reshape((r,c),order='F')\n",
    "\n",
    "        return result\n",
    "\n",
    "    def bt_wls(Y,prediction,errMat,lamda_,alpha):\n",
    "\n",
    "        (m,n)=prediction.shape\n",
    "        bands = Y.shape[2]\n",
    "    #     Y = Y.reshape((m,n,bands),order = 'F')\n",
    "\n",
    "        numClasses = errMat.shape[2]\n",
    "    #     errCube = errMat.reshape((m,n,numClasses),order = 'F')\n",
    "        errCube = errMat[...]\n",
    "        errCube = (errCube-np.min(errCube))/(np.max(errCube)-np.min(errCube))\n",
    "\n",
    "        guidanceImage = applyPCA(Y,1)[0].reshape((m,n)) \n",
    "\n",
    "        #guidanceImage = np.mean(np.log(Y.astype(np.double)+0.0001),2)\n",
    "        for i in range(numClasses):\n",
    "            slc = errCube[:,:,i]\n",
    "            slc[np.logical_not(prediction==i+1)]=0\n",
    "            slc = wls_filter(slc,lamda_,alpha,guidanceImage)\n",
    "            errCube[:,:,i]=slc\n",
    "\n",
    "        new_prediction = np.argmax(errCube,2)+1\n",
    "        new_prediction[prediction==0] = 0\n",
    "        return new_prediction   \n",
    "\n",
    "\n",
    "    smoothed_output = bt_wls(X,pred_map,pred_proba,wls_lambda,wls_alpha)\n",
    "\n",
    "    return (np.sum(pred_map[y!=0]==y[y!=0])/yPatches[yPatches!=0].shape[0],np.sum(smoothed_output[y!=0]==y[y!=0])/yPatches[yPatches!=0].shape[0])\n",
    "#         print(\"accuracy without filtering : {}\".format((np.sum(pred_map[y!=0]==y[y!=0])/yPatches[yPatches!=0].shape[0])))\n",
    "#         print(\"accuracy with filtering : {}\".format((np.sum(smoothed_output[y!=0]==y[y!=0])/yPatches[yPatches!=0].shape[0])))\n",
    "        \n",
    "#         print(\"accuracy without filtering : {}\".format((np.sum(pred_map[y!=0]==y[y!=0])/yPatches[yPatches!=0].shape[0])),file = report)\n",
    "#         print(\"accuracy with filtering : {}\".format((np.sum(smoothed_output[y!=0]==y[y!=0])/yPatches[yPatches!=0].shape[0])),file = report)\n",
    "    \n",
    " #%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nc:16  ws:5 tr:0.85     \t##\twof: 0.15025856181090838    wf: 0.16351839203824764\t##\twof: 0.13440335642501705    wf: 0.14693140794223827\t\n",
      "nc:16  ws:5 tr:0.9     \t##\twof: 0.13848180310274172    wf: 0.15533222753439363\t##\twof: 0.1129573616938238    wf: 0.1171431359156991\t\n",
      "nc:16  ws:5 tr:0.95     \t##\twof: 0.07962728071031319    wf: 0.08479851692848082\t##\twof: 0.08313006146941164    wf: 0.08004683383744755\t\n",
      "nc:16  ws:5 tr:0.99     \t##\twof: 0.00825446385013172    wf: 0.0018245682505610302\t##\twof: 0.017143135915699093    wf: 0.012040199043809152\t\n",
      "nc:16  ws:7 tr:0.85     \t##\twof: 0.168962825641526    wf: 0.1828568640843009\t##\twof: 0.1528149087715875    wf: 0.16801639184310663\t\n",
      "nc:16  ws:7 tr:0.9     \t##\twof: 0.15552736852375842    wf: 0.16560640062445117\t##\twof: 0.14331154258952092    wf: 0.15290272221680162\t\n",
      "nc:16  ws:7 tr:0.95     \t##\twof: 0.12074348716947995    wf: 0.12741730900575665\t##\twof: 0.11103522294858034    wf: 0.1171919211630403\t\n",
      "nc:16  ws:7 tr:0.99     \t##\twof: 0.03176895306859206    wf: 0.0357010440042931\t##\twof: 0.031349399941457703    wf: 0.0417504146746024\t\n",
      "nc:16  ws:9 tr:0.85     \t##\twof: 0.18023221777734413    wf: 0.18988193970143427\t##\twof: 0.16445506878719876    wf: 0.176661137671968\t\n",
      "nc:16  ws:9 tr:0.9     \t##\twof: 0.1707971509415553    wf: 0.1830910332715387\t##\twof: 0.15309786320616645    wf: 0.1632939799004781\t\n",
      "nc:16  ws:9 tr:0.95     \t##\twof: 0.1353400331739682    wf: 0.14664845350765926\t##\twof: 0.11786515757634893    wf: 0.12730022441213779\t\n",
      "nc:16  ws:9 tr:0.99     \t##\twof: 0.056239633134939994    wf: 0.0642501707483657\t##\twof: 0.04939018440823495    wf: 0.06488437896380135\t\n",
      "nc:32  ws:5 tr:0.85     \t##\twof: 0.15218070055615182    wf: 0.15906917748072982\t##\twof: 0.13895989852668553    wf: 0.15349790223436433\t\n",
      "nc:32  ws:5 tr:0.9     \t##\twof: 0.13050053663772077    wf: 0.14001365986925554\t##\twof: 0.11875304907795883    wf: 0.12289979510196117\t\n",
      "nc:32  ws:5 tr:0.95     \t##\twof: 0.08510098546199629    wf: 0.0897160698604742\t##\twof: 0.08875012196311835    wf: 0.09191140599082838\t\n",
      "nc:32  ws:5 tr:0.99     \t##\twof: 0.022607083617913942    wf: 0.033437408527661236\t##\twof: 0.014264806322568055    wf: 0.01380622499756074\t\n",
      "nc:32  ws:7 tr:0.85     \t##\twof: 0.17776368426187922    wf: 0.19018440823494975\t##\twof: 0.16165479558981363    wf: 0.17557810518099326\t\n",
      "nc:32  ws:7 tr:0.9     \t##\twof: 0.16020099521904577    wf: 0.16875792760269295\t##\twof: 0.15056103034442386    wf: 0.1630110254658991\t\n",
      "nc:32  ws:7 tr:0.95     \t##\twof: 0.12128012489023318    wf: 0.1285979119914138\t##\twof: 0.11984583861840178    wf: 0.13100790321006928\t\n",
      "nc:32  ws:7 tr:0.99     \t##\twof: 0.03876475753732071    wf: 0.05341008878915017\t##\twof: 0.03094936091325983    wf: 0.0429700458581325\t\n",
      "nc:32  ws:9 tr:0.85     \t##\twof: 0.18774514586788954    wf: 0.19391160113181774\t#"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-4799b80b7d7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'#'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                             \u001b[0mt1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumComponents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindowSize\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mws\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtestRatio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdost_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwls_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwls_lambda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m                             \u001b[0mwof\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                             \u001b[0mwf\u001b[0m\u001b[1;33m+=\u001b[0m \u001b[0mt2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-8f3437be6fc0>\u001b[0m in \u001b[0;36mclassifier\u001b[1;34m(X, y, numComponents, windowSize, testRatio, dost_, wls_alpha, wls_lambda, report, index)\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m              \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m             \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRemoteMonitor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'http://localhost:8888'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;31m#                 keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         ]\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X,y = loadIndianPinesData()\n",
    "    wls_alpha,wls_lambda = 1.2,0.8\n",
    "    for nc in [16,32,64,80,128]:\n",
    "        for ws in [5,7,9]:\n",
    "            for tr in [0.85,0.9,0.95,0.99]:\n",
    "                print(\"nc:{}  ws:{} tr:{}     \".format(nc,ws,tr),end='\\t')\n",
    "                for d_ in [True,False]:\n",
    "                    wof,wf = 0,0\n",
    "                    reportpath = \"../reports/conv_{}_{}_{}_{}_{}_{}.txt\".format(nc,ws,tr,d_,wls_alpha,wls_lambda)\n",
    "                    with open(reportpath,'w+') as report:\n",
    "                        print(\"\"\"         ------------------Iteration Info : ----------------------- \\n  \n",
    "                                    Shape of X :    {}\\n\n",
    "                                    Shape of y :    {}\\n\n",
    "                                    numComponents : {}\\n\n",
    "                                    windowSize :    {}\\n\n",
    "                                    testRatio :     {}\\n\n",
    "                                    use dost :      {}\\n\n",
    "                                    wls_alpha :     {}\\n\n",
    "                                    wls_lambda :    {}\\n\n",
    "                            \"\"\".format(X.shape,y.shape,nc,ws,tr,d_,wls_alpha,wls_lambda),file = report)\n",
    "                        for i in range(1,3):\n",
    "                            print('#',end='')\n",
    "                            t1,t2 = classifier(X,y,numComponents = nc, windowSize =ws,testRatio = tr,dost_ = d_,wls_alpha=1.2,wls_lambda=0.8,report=report,index=i)\n",
    "                            wof+=t1\n",
    "                            wf+= t2\n",
    "                        wof/= 10\n",
    "                        wf/=10\n",
    "                        print('\\t',end='')\n",
    "                        print(\"wof: {}    wf: {}\".format(wof,wf),end = '\\t')\n",
    "                        print(\"Accurcay without filter = {}\".format(wof),file=report)\n",
    "                        print(\"Accurcay with filter = {}\".format(wf),file=report)\n",
    "                       \n",
    "                \n",
    "                print('')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
